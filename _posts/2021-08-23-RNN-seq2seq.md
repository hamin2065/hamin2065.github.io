---
layout: post
title:  "[RNN] Seq2Seq"
subtitle:   
categories:  ai
tags: season-2
---

# Seq2Seq
Seq2Seq 모델은 seq를 입력받아서 seq를 출력하는 것이다. 

![seq2seq](/assets/img/posts/seq2seq.png)

그렇다면 이 모델은 기존 RNN과 뭐가 다를까?

보통 seq2seq가 잘 작동하는 예시로는 번역, 또는 챗봇이 있다.

![chatbot](/assets/img/posts/chatbot.png)

위 챗봇에서 두 번째 대화에서 RNN모델을 적용하면 RNN은 들어오는 입력마다 결과를 출력하기 때문에 날씨가 좋다는 내용에 관해서 이미 긍정적인 답변을 출력해 놓을 것이다. 마지막 단어인 `sad`가 나오기 전까지는 해야되는 대답과 굉장히 거리가 먼 결과를 가져올 것이다.

그래서 하나의 문장을 끝까지 듣고 답하는 모델을 고안해낸 것이 `Seq2Seq`모델이다.

![encoder-decoder](/assets/img/posts/encoder-decoder.png)

Seq2Seq의 대표적인 형태는 `Encoder-Decoder`형태이다. `Encoder`에서는 입력된 단어형태로 Sequence가 나오고 그 문장을 벡터형태로 압축해준다. 그 압축된 벡터를 `Decoder`로 넘겨주고 그 벡터는 첫 셀의 `hidden state`로 들어가게 된다.

# Apply to Pytorch

```py
import random
import torch
import torch.nn as nn
from torch import optim
```
코드의 첫 부분으로 필요한 모듈을 불러온다.

이 학습시키는 과정은 번역을 하기 위한 코드인데 원본인 source text를 target text로 바꿔야한다. 여기서는 source text는 영어, target text는 한국어로 구성되어 있다.
```py
SOURCE_MAX_LENGTH = 10
TARGET_MAX_LENGTH = 12

# 학습할 dataset 나눠주기
load_pairs, load_source_vocab, load_target_vocab = preprocess(raw, SOURCE_MAX_LENGTH, TARGET_MAX_LENGTH)
print(random.choice(load_pairs))

enc_hidden_size = 16
dec_hidden_size = enc_hidden_size
enc = Encoder(load_source_vocab.n_vocab, enc_hidden_size).to(device)
dec = Decoder(dec_hidden_size, load_target_vocab.n_vocab).to(device)

train(load_pairs, load_source_vocab, load_target_vocab, enc, dec, 5000, print_every=1000)
evaluate(load_pairs, load_source_vocab, load_target_vocab, enc, dec, TARGET_MAX_LENGTH)
```

데이터를 선언하는 부분
```py
torch.manual_seed(0)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# source text와 target text는 <tab>으로 구분
raw = ["I feel hungry.	나는 배가 고프다.",
       "Pytorch is very easy.	파이토치는 매우 쉽다.",
       "Pytorch is a framework for deep learning.	파이토치는 딥러닝을 위한 프레임워크이다.",
       "Pytorch is very clear to use.	파이토치는 사용하기 매우 직관적이다."]
```
```py
# fix token for "start of sentence" and "end of sentence"
SOS_token = 0
EOS_token = 1
```

데이터를 나누는 preprocess 함수를 작성해준다.
```py
def preprocess(corpus, source_max_length, target_max_length):
    print("reading corpus...")
    pairs = []
    for line in corpus:
        pairs.append([s for s in line.strip().lower().split("\t")])
    print("Read {} sentence pairs".format(len(pairs)))

    pairs = [pair for pair in pairs if filter_pair(pair, source_max_length, target_max_length)]
    print("Trimmed to {} sentence pairs".format(len(pairs)))

    source_vocab = Vocab()
    target_vocab = Vocab()

    print("Counting words...")
    for pair in pairs:
        source_vocab.add_vocab(pair[0])
        target_vocab.add_vocab(pair[1])
    print("source vocab size =", source_vocab.n_vocab)
    print("target vocab size =", target_vocab.n_vocab)

    return pairs, source_vocab, target_vocab
```
`Encoder class`
```py
class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(Encoder, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)

    # x를 embedding 시키고 gru에 넣어서 통과시킴
    def forward(self, x, hidden):
        x = self.embedding(x).view(1, 1, -1)
        x, hidden = self.gru(x, hidden)
        return x, hidden
```
단어가 원핫 인코딩을 통해서 들어오면 `Embedding`이라는 메소드에서 hidden_size만큼의 벡터로 줄이는 일을 한다.

세로로긴 단어가 들어오면 가로로긴 Embedding과 곱해서 우리가 원하는 사이즈로 만든다. 그리고 만든걸 또 `GRU`에 넣어서 처리를 하면 된다.

`Decoder class`
```py
class Decoder(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(Decoder, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, x, hidden):
        x = self.embedding(x).view(1, 1, -1)
        x, hidden = self.gru(x, hidden)
        x = self.softmax(self.out(x[0]))
        return x, hidden
```
Decoder는 Encoder와 별로 다르지 않다. embedding과 gru는 공통적이고 out과 softmax를 추가적으로 통과시켜주면 된다.

Embedding과 GRU를 통과하면서 줄어든 사이즈를 다시 원래대로 만들기 위해서 out을 써준다.

필요한 함수와 클래스를 모두 선언하고 학습을 시켜준다.

문장이 들어오면 원핫인코딩으로 바꿔서 그걸 텐서로 바꿔주는 역할을 한다.
```py
def tensorize(vocab, sentence):
    indexes = [vocab.vocab2index[word] for word in sentence.split(" ")]
    indexes.append(vocab.vocab2index["<EOS>"])
    return torch.Tensor(indexes).long().to(device).view(-1, 1)
```
```py
def train(pairs, source_vocab, target_vocab, encoder, decoder, n_iter, print_every=1000, learning_rate=0.01):
    loss_total = 0

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)

    training_batch = [random.choice(pairs) for _ in range(n_iter)]
    training_source = [tensorize(source_vocab, pair[0]) for pair in training_batch]
    training_target = [tensorize(target_vocab, pair[1]) for pair in training_batch]

    criterion = nn.NLLLoss()

    for i in range(1, n_iter + 1):
        source_tensor = training_source[i - 1]
        target_tensor = training_target[i - 1]

        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)

        encoder_optimizer.zero_grad()
        decoder_optimizer.zero_grad()

        source_length = source_tensor.size(0)
        target_length = target_tensor.size(0)

        loss = 0

        for enc_input in range(source_length):
            _, encoder_hidden = encoder(source_tensor[enc_input], encoder_hidden)

        decoder_input = torch.Tensor([[SOS_token]]).long().to(device)
        decoder_hidden = encoder_hidden # connect encoder output to decoder input

        for di in range(target_length):
            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # teacher forcing

        loss.backward()

        encoder_optimizer.step()
        decoder_optimizer.step()

        loss_iter = loss.item() / target_length
        loss_total += loss_iter

        if i % print_every == 0:
            loss_avg = loss_total / print_every
            loss_total = 0
            print("[{} - {}%] loss = {:05.4f}".format(i, i / n_iter * 100, loss_avg))
```