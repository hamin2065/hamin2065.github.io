---
layout: post
title:  ReLu
subtitle:   
categories:  ai
tags: season-1
---

# Sigmoid의 문제점
0~1사이의 값인 sigmoid함수 값을 계속 곱해주게 되면 아주 작은 숫자, 즉 0에 가까운 값들이 계속 곱해지고 그 결과로 굉장히 작은 값을 갖게 된다.

그렇게 되면 기울기가 점점 사라지는 문제가 생기게 되는데 이 현상을 __Vanishing Gradient__ 라고 부른다.

![vanishing_gradient](https://t1.daumcdn.net/cfile/tistory/99B70D395AD8972830)

# ReLu
이 문제를 해결하기 위해서 등장한 함수가 __ReLu__ 함수이다.

![ReLu](https://t1.daumcdn.net/cfile/tistory/99EF023C5AD898222E)
ReLu 함수는 아주 단순하게 0보다 작은 경우는 0으로 나타내고 0보다 큰 경우는 그 값을 그대로 나타낸다.

![ReLu_Cost_Function](https://t1.daumcdn.net/cfile/tistory/9906E73D5AD898F230)
ReLu 함수의 cost함수를 보면 sigmoid 함수보다 확연하게 올바른 학습을 하는 것을 확인할 수 있다.