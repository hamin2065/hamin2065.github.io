---
layout: post
title: Lab-09-01 ReLU
color: rgb(242,85,44)
tags: [AI, PyTorch]
---

# 학습목표
ReLU 활성화 함수에 대해 알아본다.
# 핵심키워드
- ReLU 
- Sigmoid
- Optimizer
![coding](../../../assets/img/posts/Lab-09-1ReLU-01.jpg)
![coding](../../../assets/img/posts/Lab-09-1ReLU-02.jpg)

# Problem of Sigmoid
- __backpropagation__ : 2단, 3단 정도의 레이어는 학습이 잘 되지만, 9단, 10단으로 넘어가면서부터는 학습이 제대로 이루어지지 않는다. 레이어가 많을 경우 각각의 단계의 값을 미분해서 최초 레이어까지 결과 값을 전달해가게 되는데 내부의 hidden layer들이 모두 sigmoid함수로 이루어져 있다면 계산한 값은 모두 0과 1사이에 있다. 
- __vanishing Gradient__ : 따라서 여러 레이어를 가지고 있을 떄, 최초 입력 값은 각각의 레이어에서 나온 값들을 곱해준 만큼의 결과에 영향을 주는 것이므로 최종 미분값은 결국 0에 가까운 값이 될 수 밖에 없다. 이를 경사도(기울기)가 사라지는 현상으로 본다. 최초 입력 값이 최종 결과 값에 별로 영향을 끼치지 않는다는 결론으로 수렴하게 되는 것이다. 

시그모이드 함수는 `0 < n < 1` 사이의 값만 다루므로 결국 chain룰을 이용해서 계속 값을 곱해나간다고 했을 때 결과 값이 0에 수렴할 수 밖에 없다는 한계를 가지고 있으므로 나중에는 1보다 작아지지 않게 하기 위한 대안으로 ReLU함수를 사용하게 된다. 
![coding](../../../assets/img/posts/Lab-09-1ReLU-03.jpg)
![coding](../../../assets/img/posts/Lab-09-1ReLU-04.jpg)
![coding](../../../assets/img/posts/Lab-09-1ReLU-05.jpg)
# ReLU
내부 hidden layer을 활성화시키는 함수로 sigmoid를 사용하지 않고 ReLU라는 활성화 함수를 사용하게 되는데, 이 함수는 0보다 작은 값이 나온 경우 0을 반환하고 0보다 큰 값이 나온 경우 그 값을 그대로 반환하는 함수이다. 
![coding](../../../assets/img/posts/Lab-09-1ReLU-06.jpg)

# Optimizer in Pytorch
![coding](../../../assets/img/posts/Lab-09-1ReLU-07.jpg)
![coding](../../../assets/img/posts/Lab-09-1ReLU-08.jpg)
![coding](../../../assets/img/posts/Lab-09-1ReLU-09.jpg)
![coding](../../../assets/img/posts/Lab-09-1ReLU-10.jpg)
![coding](../../../assets/img/posts/Lab-09-1ReLU-11.jpg)
![coding](../../../assets/img/posts/Lab-09-1ReLU-12.jpg)
![coding](../../../assets/img/posts/Lab-09-1ReLU-13.jpg)

참고

https://medium.com/@kmkgabia/ml-sigmoid-%EB%8C%80%EC%8B%A0-relu-%EC%83%81%ED%99%A9%EC%97%90-%EB%A7%9E%EB%8A%94-%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-c65f620ad6fd